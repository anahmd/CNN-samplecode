{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_58uI_DJdkuu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "cv1L-sVmq1ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from livelossplot import PlotLossesKeras\n"
      ],
      "metadata": {
        "id": "d93j_jyqkqZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_cv\n",
        "from keras_cv import bounding_box\n",
        "import os\n",
        "from luketils import visualization"
      ],
      "metadata": {
        "id": "rDIXqvJHgus_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "qA0ydIMHiRax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMHb9n_N4pI7",
        "outputId": "25e917af-f816-4585-b8ee-854e8bf54edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.8/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_generator = ImageDataGenerator(rotation_range=90, \n",
        "                                     brightness_range=[0.1, 0.7],\n",
        "                                     width_shift_range=0.5, \n",
        "                                     height_shift_range=0.5,\n",
        "                                     horizontal_flip=True, \n",
        "                                     vertical_flip=True,\n",
        "                                     validation_split=0.15,\n",
        "                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n",
        "\n",
        "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing"
      ],
      "metadata": {
        "id": "22AStRhgeCID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_dir = Path('/content/drive/MyDrive/VOC2008/')"
      ],
      "metadata": {
        "id": "i_2OLEokn1bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDdJ21s6uHXH",
        "outputId": "1d6dfbe3-0807-4e14-daa1-e3dd141ea78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/VOC2008')"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_subset = sorted(os.listdir(download_dir/'JPEGImages'))[:20]"
      ],
      "metadata": {
        "id": "7BZl6uKKotPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_subset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qn4OgICFBSw",
        "outputId": "4690331f-1462-4c8b-c87b-5a3f80388b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2007_001185.jpg',\n",
              " '2007_001225.jpg',\n",
              " '2007_001239.jpg',\n",
              " '2007_001284.jpg',\n",
              " '2007_001288.jpg',\n",
              " '2007_001289.jpg',\n",
              " '2007_001299.jpg',\n",
              " '2007_001311.jpg',\n",
              " '2007_001321.jpg',\n",
              " '2007_001340.jpg',\n",
              " '2007_001377.jpg',\n",
              " '2007_001397.jpg',\n",
              " '2007_001408.jpg',\n",
              " '2007_001416.jpg',\n",
              " '2007_001420.jpg',\n",
              " '2007_001423.jpg',\n",
              " '2007_001430.jpg',\n",
              " '2007_001439.jpg',\n",
              " '2007_001457.jpg',\n",
              " '2007_001458.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_generator = ImageDataGenerator(\n",
        "    rescale=1/255.,              # normalize pixel values between 0-1\n",
        "    brightness_range=[0.1, 0.7], # specify the range in which to decrease/increase brightness\n",
        "    width_shift_range=0.5,       # shift the width of the image 50%\n",
        "    rotation_range=90,           # random rotation by 90 degrees\n",
        "    horizontal_flip=True,        # 180 degree flip horizontally\n",
        "    vertical_flip=True,          # 180 degree flip vertically\n",
        "    validation_split=0.15        # 15% of the data will be used for validation at end of each epoch\n",
        ")"
      ],
      "metadata": {
        "id": "R5okWpG-NHaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.mkdir(download_dir/'food-101')\n",
        "#os.mkdir(download_dir/'food-101/train')\n",
        "#os.mkdir(download_dir/'food-101/valid')\n",
        "\n",
        "train_data_dir = download_dir/'food-101/train'\n",
        "test_data_dir = download_dir/'food-101/test'\n",
        "valid_data_dir = download_dir/'food-101/valid'\n",
        "\n",
        "class_id = [\n",
        "    \"Aeroplane\",\n",
        "    \"Bicycle\",\n",
        "    \"Bird\",\n",
        "    \"Boat\",\n",
        "    \"Bottle\",\n",
        "    \"Bus\",\n",
        "    \"Car\",\n",
        "    \"Cat\",\n",
        "    \"Chair\",\n",
        "    \"Cow\",\n",
        "    \"Dining Table\",\n",
        "    \"Dog\",\n",
        "    \"Horse\",\n",
        "    \"Motorbike\",\n",
        "    \"Person\",\n",
        "    \"Potted Plant\",\n",
        "    \"Sheep\",\n",
        "    \"Sofa\",\n",
        "    \"Train\",\n",
        "    \"Tvmonitor\",\n",
        "]\n",
        "\n",
        "class_subset = sorted(os.listdir(download_dir/'JPEGImages'))[:20] # Using only the first 10 classes\n",
        "\n",
        "traingen = train_generator.flow_from_directory('/content/drive/MyDrive/VOC2008/JPEGImages',\n",
        "                                               target_size=(512, 512),\n",
        "                                               class_mode='categorical',\n",
        "                                               classes=class_subset,\n",
        "                                               subset='training',\n",
        "                                               batch_size=BATCH_SIZE, \n",
        "                                               shuffle=True,\n",
        "                                               seed=42)\n",
        "\n",
        "validgen = train_generator.flow_from_directory('/content/drive/MyDrive/VOC2008/food-101/train/',\n",
        "                                               target_size=(512, 512),\n",
        "                                               class_mode='categorical',\n",
        "                                               classes=class_subset,\n",
        "                                               subset='validation',\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               seed=42)\n",
        "\n",
        "testgen = test_generator.flow_from_directory('//content/drive/MyDrive/VOC2008/food-101/test/',\n",
        "                                             target_size=(512, 512),\n",
        "                                             class_mode=None,\n",
        "                                             classes=class_id,\n",
        "                                             batch_size=1,\n",
        "                                             shuffle=False,\n",
        "                                             seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkDtCytyDSmB",
        "outputId": "c817084c-4b28-4f1f-96d4-2bf5ad6c6509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 20 classes.\n",
            "Found 0 images belonging to 20 classes.\n",
            "Found 0 images belonging to 20 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiles a model integrated with VGG16 pretrained layers\n",
        "input_shape: tuple - the shape of input images (width, height, channels)\n",
        "n_classes: int - number of classes for the output layer\n",
        "optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
        "fine_tune: int - The number of pre-trained layers to unfreeze.\n",
        "If set to 0, all pretrained layers will freeze during training"
      ],
      "metadata": {
        "id": "9XFkLdHVvhaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
        "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
        "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
        "    conv_base = VGG16(include_top=False,\n",
        "                     weights='imagenet', \n",
        "                     input_shape=input_shape)\n",
        "    \n",
        "    # Defines how many layers to freeze during training.\n",
        "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
        "    # depending on the size of the fine-tuning parameter.\n",
        "    if fine_tune > 0:\n",
        "        for layer in conv_base.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
        "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
        "    top_model = conv_base.output\n",
        "    top_model = Flatten(name=\"flatten\")(top_model)\n",
        "    top_model = Dense(4096, activation='relu')(top_model)\n",
        "    top_model = Dense(1072, activation='relu')(top_model)\n",
        "    top_model = Dropout(0.2)(top_model)\n",
        "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
        "    \n",
        "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
        "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
        "\n",
        "    # Compiles the model for training.\n",
        "    model.compile(optimizer=optimizer, \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "WtIRyMBHigPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.traitlets import validate\n",
        "input_shape = (512, 512, 3)\n",
        "optim_1 = Adam(learning_rate=0.001)\n",
        "n_classes=20\n",
        "\n",
        "n_epochs = BATCH_SIZE\n",
        "\n",
        "# First we'll train the model without Fine-tuning\n",
        "vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkQtBIt-ipDG",
        "outputId": "5d855dd5-4bb8-486d-a7be-29bad6a532a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from livelossplot.inputs.keras import PlotLossesCallback\n",
        "\n",
        "plot_loss_1 = PlotLossesCallback()\n",
        "\n",
        "# ModelCheckpoint callback - save best weights\n",
        "tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n",
        "                                  save_best_only=True,\n",
        "                                  verbose=1)\n",
        "\n",
        "# EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss',\n",
        "                           patience=10,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='min')"
      ],
      "metadata": {
        "id": "y1ZkxQwVj7JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_history = vgg_model.fit(traingen,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            epochs=EPOCHS,\n",
        "                            validation_data=validgen,\n",
        "                            steps_per_epoch=traingen.samples,\n",
        "                            validation_steps=validgen.samples,\n",
        "                            callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n",
        "                            verbose=1)"
      ],
      "metadata": {
        "id": "EQQMEf8LnEGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "vgg_model.load_weights('tl_model_v1.weights.best.hdf5') # initialize the best trained weights\n",
        "\n",
        "true_classes = testgen.classes\n",
        "class_indices = traingen.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "vgg_preds = vgg_model.predict(testgen)\n",
        "vgg_pred_classes = np.argmax(vgg_preds, axis=1)"
      ],
      "metadata": {
        "id": "TVGBenwevOUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vgg_acc = accuracy_score(true_classes, vgg_pred_classes)\n",
        "print(\"VGG16 Model Accuracy without Fine-Tuning: {:.2f}%\".format(vgg_acc * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFyYwFcJvYz6",
        "outputId": "4097cb35-7bf2-4254-92e4-dd0513ad4706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16 Model Accuracy with Fine-Tuning: 88.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset our image data generators\n",
        "traingen.reset()\n",
        "validgen.reset()\n",
        "testgen.reset()\n",
        "\n",
        "# Use a smaller learning rate\n",
        "optim_2 = Adam(lr=0.0001)\n",
        "\n",
        "# Re-compile the model, this time leaving the last 2 layers unfrozen for Fine-Tuning\n",
        "vgg_model_ft = create_model(input_shape, n_classes, optim_2, fine_tune=2)"
      ],
      "metadata": {
        "id": "O6kk9utvv3SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_2 = PlotLossesCallback()\n",
        "\n",
        "# Retrain model with fine-tuning\n",
        "vgg_ft_history = vgg_model_ft.fit(traingen,\n",
        "                                  batch_size=BATCH_SIZE,\n",
        "                                  epochs=n_epochs,\n",
        "                                  validation_data=validgen,\n",
        "                                  steps_per_epoch=n_steps, \n",
        "                                  validation_steps=n_val_steps,\n",
        "                                  callbacks=[tl_checkpoint_1, early_stop, plot_loss_2],\n",
        "                                  verbose=1)\n"
      ],
      "metadata": {
        "id": "dOQ2j9X9v-MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "vgg_model_ft.load_weights('tl_model_v1.weights.best.hdf5') # initialize the best trained weights\n",
        "\n",
        "vgg_preds_ft = vgg_model_ft.predict(testgen)\n",
        "vgg_pred_classes_ft = np.argmax(vgg_preds_ft, axis=1)"
      ],
      "metadata": {
        "id": "rle7EbE4wCRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_acc_ft = accuracy_score(true_classes, vgg_pred_classes_ft)\n",
        "print(\"VGG16 Model Accuracy with Fine-Tuning: {:.2f}%\".format(vgg_acc_ft * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pKA-OSZwP_j",
        "outputId": "fb53f754-a652-4769-bdcc-6699adc66b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15400 images belonging to 20 classes.\n",
            "From Scratch Model Accuracy with Fine-Tuning: 88.26%\n"
          ]
        }
      ]
    }
  ]
}